{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### takes long time to run\n",
    "#from datasets import load_dataset\n",
    "\n",
    "#split = \"train\"\n",
    "#filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "#data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "#filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.26% of data after filtering resulting in 6 GB and consists of 600,000 Python scripts\n",
    "\n",
    "filtering the fulldataset takes 2-3h depending on your machine and bandwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Christian/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Found cached dataset json (C:/Users/Christian/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-valid-65557c3279496c87/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### this is the prefiltered dataset to skip filtering whole dataset\n",
    "#from datasets import load_dataset, DatasetDict\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining the language model will take a while. We suggest that you first run the training loop on a sample of the data by uncommenting the two partial lines above, and make sure that the training successfully completes and the models are stored. Nothing is more frustrating than a training run failing at the last step because you forgot to create a folder or because there’s a typo at the end of the training loop!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 34\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained('huggingface-course/code-search-net-tokenizer')\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Christian\\.cache\\huggingface\\datasets\\huggingface-course___json\\huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-cf462dbd021072f8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Christian\\.cache\\huggingface\\datasets\\huggingface-course___json\\huggingface-course--codeparrot-ds-valid-65557c3279496c87\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-5580b16c22b3b9e6.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 16702061\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 93164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids'],\n",
    "#        num_rows: 16702061\n",
    "#    })\n",
    "#    valid: Dataset({\n",
    "#        features: ['input_ids'],\n",
    "#        num_rows: 93164\n",
    "#    })\n",
    "#})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion tokens in total. For reference, OpenAI’s GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializing a new model\n",
    "Our first step is to freshly initialize a GPT-2 model. We’ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the bos and eos (beginning and end of sequence) token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 size: 124.2M parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "\n",
    "#input_ids shape: torch.Size([5, 128])\n",
    "#attention_mask shape: torch.Size([5, 128])\n",
    "#labels shape: torch.Size([5, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd8ac6dbeb542dcbf8bedf99bd8cc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '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' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Christian\\Desktop\\datasci_gpt\\load.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Christian/Desktop/datasci_gpt/load.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m 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\n",
      "\u001b[1;31mNameError\u001b[0m: name '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' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. configure training arguements and fire up the Trainer\n",
    "2. useing cosine learning rate schedule with some warmup\n",
    "3. effective batch size of 256 (per_device_train_batch_size*gradient_accumulation_steps)\n",
    "# gradient accumulation is used when a single batch does not fit in memory and incrementally builds up the gradient through several forward/backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cdreetz/casual-hug/e/CAS-11\n"
     ]
    }
   ],
   "source": [
    "from transformers.integrations import NeptuneCallback\n",
    "import neptune\n",
    "\n",
    "run = neptune.init_run(project=\"cdreetz/casual-hug\",\n",
    "                       api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxMmNlMjY1NS1iZDRhLTQ1MjEtYjVhNy0zNDk3Njc4NGU2YjEifQ==\",\n",
    "                       capture_hardware_metrics=True,\n",
    "                       capture_stderr=True,\n",
    "                       capture_stdout=True)\n",
    "\n",
    "neptune_callback = NeptuneCallback(run=run,\n",
    "                                   log_parameters=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\Desktop\\datasci_gpt\\codeparrot-ds2 is already a clone of https://huggingface.co/cdreetz/codeparrot-ds2. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds2\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=200,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    callbacks=[neptune_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1498 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011956930160522461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 130484,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f5087dca9847da908ad78fa94b1968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Christian\\Desktop\\datasci_gpt\\load.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Christian/Desktop/datasci_gpt/load.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1669\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1938\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1940\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1942\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1943\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1944\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1945\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1946\u001b[0m ):\n\u001b[0;32m   1947\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2742\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m-> 2745\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2746\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[0;32m   2747\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[8.82269263e-01 9.15003955e-01 3.82863760e-01 ... 2.69494832e-01\n",
      "    3.58812630e-01 1.99363768e-01]\n",
      "   [5.47191560e-01 6.16043806e-03 9.51554537e-01 ... 9.10269439e-01\n",
      "    6.44015670e-01 7.07106769e-01]\n",
      "   [6.58130586e-01 4.91302013e-01 8.91304135e-01 ... 1.59144104e-01\n",
      "    7.65289068e-01 2.97897756e-01]\n",
      "   ...\n",
      "   [8.02882731e-01 2.66210496e-01 2.61398315e-01 ... 6.68269873e-01\n",
      "    6.77897573e-01 8.37045908e-02]\n",
      "   [1.49900913e-02 2.40555465e-01 8.42273831e-01 ... 4.93099749e-01\n",
      "    9.57616389e-01 1.99889958e-01]\n",
      "   [5.03931105e-01 7.37799764e-01 1.54821873e-01 ... 3.01824689e-01\n",
      "    6.30125940e-01 6.88570201e-01]]]\n",
      "\n",
      "\n",
      " [[[2.36630499e-01 4.21047211e-03 7.61717200e-01 ... 1.94603682e-01\n",
      "    2.53947854e-01 5.96131444e-01]\n",
      "   [6.35630608e-01 6.92236125e-01 7.74437606e-01 ... 4.58340883e-01\n",
      "    6.07877910e-01 2.25802660e-01]\n",
      "   [6.44235015e-01 1.17883086e-02 1.42245770e-01 ... 9.18389320e-01\n",
      "    8.87405157e-01 6.51075661e-01]\n",
      "   ...\n",
      "   [3.88001800e-01 3.18637729e-01 6.96419597e-01 ... 6.83074176e-01\n",
      "    2.70447612e-01 9.29053187e-01]\n",
      "   [7.38626242e-01 8.28447342e-01 5.66039264e-01 ... 3.46503556e-01\n",
      "    2.41869271e-01 3.39226484e-01]\n",
      "   [3.21725726e-01 9.78320003e-01 6.91785991e-01 ... 3.51518273e-01\n",
      "    4.98172998e-01 6.60500705e-01]]]\n",
      "\n",
      "\n",
      " [[[4.89014268e-01 5.23147404e-01 5.63343585e-01 ... 1.66441619e-01\n",
      "    6.74093425e-01 2.00133264e-01]\n",
      "   [3.42609584e-01 2.61665583e-01 1.59842908e-01 ... 3.73250544e-01\n",
      "    7.57910550e-01 6.98145092e-01]\n",
      "   [9.13646042e-01 3.97641540e-01 7.35522747e-01 ... 6.06437325e-02\n",
      "    4.83036637e-01 2.77818978e-01]\n",
      "   ...\n",
      "   [9.14242506e-01 9.86863792e-01 4.89498377e-01 ... 2.49165475e-01\n",
      "    5.63669622e-01 7.93096244e-01]\n",
      "   [7.48922348e-01 2.79208779e-01 2.73166895e-02 ... 3.17164004e-01\n",
      "    9.90685046e-01 9.06412065e-01]\n",
      "   [7.90426850e-01 5.28041124e-01 3.38113606e-01 ... 3.22536409e-01\n",
      "    2.36257017e-01 5.81930876e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.42379045e-01 7.98324645e-01 6.55941308e-01 ... 5.30312300e-01\n",
      "    9.68952060e-01 8.12058330e-01]\n",
      "   [1.09873235e-01 1.69496834e-01 1.45071507e-01 ... 8.48067820e-01\n",
      "    4.41682696e-01 5.52994967e-01]\n",
      "   [6.90706551e-01 4.68096912e-01 7.37816930e-01 ... 2.63299704e-01\n",
      "    5.78853667e-01 9.15810466e-02]\n",
      "   ...\n",
      "   [9.92587030e-01 5.79267383e-01 9.01776016e-01 ... 7.37973690e-01\n",
      "    6.89668179e-01 2.23442852e-01]\n",
      "   [8.77688169e-01 1.69428527e-01 3.45862687e-01 ... 8.43145728e-01\n",
      "    8.86538029e-02 5.30748963e-01]\n",
      "   [7.67979026e-02 7.26943016e-01 6.40492260e-01 ... 5.84131539e-01\n",
      "    6.82461262e-02 9.28841949e-01]]]\n",
      "\n",
      "\n",
      " [[[4.81670499e-02 3.83495569e-01 4.75540042e-01 ... 1.40741646e-01\n",
      "    8.11151922e-01 5.44763207e-01]\n",
      "   [7.68367350e-01 5.58454096e-01 4.56570327e-01 ... 6.22811079e-01\n",
      "    3.73236418e-01 5.59222698e-01]\n",
      "   [2.52418995e-01 8.89353752e-02 9.54107404e-01 ... 7.17451036e-01\n",
      "    1.94739997e-01 7.91853845e-01]\n",
      "   ...\n",
      "   [3.99306715e-01 7.12200284e-01 4.70273674e-01 ... 7.76064396e-02\n",
      "    3.07229996e-01 4.62298691e-01]\n",
      "   [5.18807352e-01 7.88509846e-04 3.06313932e-01 ... 4.15569663e-01\n",
      "    1.61214471e-02 1.56950474e-01]\n",
      "   [7.25690424e-01 5.51439762e-01 5.33640325e-01 ... 9.43374038e-01\n",
      "    1.30126476e-02 9.26233470e-01]]]\n",
      "\n",
      "\n",
      " [[[4.18155789e-01 8.34981263e-01 2.16695309e-01 ... 1.19665146e-01\n",
      "    1.51720226e-01 1.80013418e-01]\n",
      "   [9.34495270e-01 5.35995305e-01 6.42247260e-01 ... 5.43782532e-01\n",
      "    6.15928292e-01 4.92158949e-01]\n",
      "   [7.59376049e-01 4.36995208e-01 7.45299816e-01 ... 3.80476117e-02\n",
      "    3.27982306e-02 1.55698180e-01]\n",
      "   ...\n",
      "   [8.03481519e-01 9.91210461e-01 5.13834953e-02 ... 8.00927460e-01\n",
      "    1.48292482e-01 3.19429338e-01]\n",
      "   [6.69168949e-01 4.63396013e-01 7.82724023e-02 ... 7.55768538e-01\n",
      "    7.15762317e-01 5.62291145e-02]\n",
      "   [6.19502544e-01 4.62482691e-01 5.39095521e-01 ... 3.51094007e-01\n",
      "    9.20968533e-01 1.89816654e-01]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "test_torch = torch.rand(100,1,28,28)\n",
    "print(test_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
